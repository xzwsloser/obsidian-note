# 机器学习
## 机器学习中的基本概念
- 特征: 表示需要预测对象的特点(比如芒果的大小,形状颜色等特征信息)
- 标签: 表示最终需要预测的变量(比如芒果的甜度等信息),可以是离散的也可以是连续的
- 一个标记好特征以及标签的对象可以看成一个样本
- 一组样本构成的集合称为数据集（Data Set）．一般将数据集分为两部分： 数据集也训练集和测试集．训练集（Training Set）中的样本是用来训练模型的，也叫训练样本（Training Sample），而测试集（Test Set）中的样本是用来检验模型好坏的，也叫测试样本（Test Sample）
- 可以使用一个`D`维的向量来表示样本的特征,叫做特征向量:
$$
{\mathbf x} = [x_1 , x_2 , ...,x_D]^T
$$
- 训练集由`N`个样本组成,每一个样本都是独立同分布的,记为:
$$
{\mathcal D} =  \{({\mathbf x}^{(1)} , {\mathbf y}^{(1)}) , ({\mathbf x}^{(2)} , {\mathbf y}^{(2)})  , ... , ({\mathbf x}^{(N)} , {\mathbf y}^{(N)})\}
$$
- 给定训练集需要从函数集合
$$
\mathcal F = \{f_1(\mathbf x) , f_2(\mathbf x) ... \} 
$$
中找到一个最有的函数`f(x)`来近似表示特征向量与标签之前的关系,也就是:
$$
\widehat y = f^*(\mathbf x)
$$
或者条件概率
$$
\widehat p(y|\mathbf x) = f_y^*(\mathbf x)
$$
- 寻找着一个最优函数的过程叫做学习,一般通过学习算法来完成
- 一般可以通过测试集确定模型的准确率:
![[Pasted image 20241112142958.png]]
- 机器学习的基本流程:
![[Pasted image 20241112143034.png]]
## 机器学习的三要素
### 模型
- 也就是上面说的最优函数,一般可以从一个函数集合(这里成为假设空间中找到这一个最优模型):
$$
{\mathcal F} = \{f({\mathbf x};\theta)|\theta \in \mathbb R^D\} 
$$
- 线性模型:
$$
f(\mathbf x ; \theta) = {\mathbf \omega}^T{\mathbf x} + b
$$
- 非线性模型:
![[Pasted image 20241112143535.png]]
### 学习准则
- 一般可以通过期望风险来衡量,定义为:
$$
\mathcal R(\theta) = \mathbb E_{(x,y) - p_r(x,y)}[\mathcal L(y,f(\mathbf x;\theta))]
$$
- 其中也就是测试集中每一个点的风险函数的取值的均值
#### 损失函数
- 用于衡量模型预测与真实标签之间的差异
- 常用的损失函数:
- `01`损失函数:
![[Pasted image 20241112143953.png]]
- 平方损失函数:
![[Pasted image 20241112144020.png]]
- 交叉熵损失函数(一般用于分类问题),类似于期望
![[Pasted image 20241112144203.png]]
#### 风险最小化
- 一般利用经验风险评估模型的准确性:
![[Pasted image 20241112144351.png]]
- 同时为了防止过拟合,一般可以引入参数的正则化来限制模型的能力,这一种准册叫做结构风险最小化:
![[Pasted image 20241112144439.png]]
- 欠拟合,过拟合和正常的情况:
![[Pasted image 20241112144521.png]]

#### 优化算法
- 可以看成利用寻找最优模型的算法,是一种迭代算法(寻找经验风险`or`结构风险最小的点)
#### 梯度下降法
- 利用如下公式进行参数的迭代:
![[Pasted image 20241112144738.png]]
#### 提前停止
- 划分出验证集,每一次将得到的模型在验证集上面进行验证,如果发现验证集中的错误率不再下降就会停止,可以防止过拟合
![[Pasted image 20241112144901.png]]
#### 随机梯度下降
- 为了防止每一次迭代都需要计算训练集中每一个样本的偏导数,所以可以参用随机梯度下降法,也就是随机选择一个样本进行梯度下降:
![[Pasted image 20241112145030.png]]
#### 批量梯度下降
- 也似乎为了减少梯度下降的复杂度,每一次只是抽取一个小部分的元素
![[Pasted image 20241112145118.png]]

### 机器学习算法的种类
- 监督学习(回归,分类,结构化等问题)
- 无监督学习
- 强化学习
- 三者的差别:
![[Pasted image 20241112145443.png]]
- 交叉验证:
![[Pasted image 20241112145603.png]]
## 线性模型
### Logistic回归
- 首先回归问除了需要获取一个判别函数($f(x; \mathbf \omega) = {\mathbf \omega}^T {\mathbf x} + b$ ) 之外还需要确定一个决策函数$g(x)$ 用于把判别函数的结果映射到一个有效的类别上
- `Logistic`回归可以利用于解决二分类的问题,考虑后验概率:
$$
p(y = 1 | {\mathbf x}) = g(f({\mathbf x}; {\mathbf \omega})) 
$$
- `Logisitc`回归中使用`Logistic`函数作为激活函数,比如标签$y = 1$ 的后验概率为 
$$
p(y = 1 | x) = \sigma({\mathbf \omega}^{T}{\mathbf x}) = \frac{1}{1 + \exp(-{\mathbf \omega}^{T}{\mathbf x})} 
$$
- 这里的$\omega$ 增广权重矩阵 , $\mathbf x$ 是增广特征矩阵
- 从而可以得到:
$$
p(y = 0 | {\mathbf x}) = 1 - p(y = 1 | {\mathbf x}) 
$$
- 结合上面两个式子可以得到:
$$
{\mathbf \omega}^{T}{\mathbf x} = log{\frac{p(y = 1 | {\mathbf x})}{p(y = 0 | {\mathbf x})}}
$$
- 参数学习的过程中使用交叉熵函数作为损失函数并且利用梯队下降算法来优化参数
- 后验概率为:
$$
\widehat y^{(n)} = \sigma({\mathbf \omega}^{T}{\mathbf x}^{{n}})  
$$
- 所以可以得到:
$$
p_r(y^{n} = 1 | {\mathbf x}^{n}) = y^{(n)}  
$$
$$
p_r(y^{n} = 0 | {\mathbf x}^{n}) = 1- y^{(n)}
$$
![[Pasted image 20241111214717.png]]
### Softmax回归
- `Softmax`中预测最终结果属于类别`c`的条件概率为,其中`x`为样本:
$$
p(y = c | \mathbf x) = softmax({\mathbf \omega}_c^T{\mathbf x}) = \frac{exp(\mathbf \omega_c^T{\mathbf x})}{\sum_\limits {c' = 1}^{C} exp({\mathbf \omega}_{c'}^T{\mathbf x})}
$$
- 决策函数使用如下函数:
![[Pasted image 20241112152214.png]]
- 但种类为`2`的时候就会退化成`Logistic`回归
- 利用向量形式可以表示为:
![[Pasted image 20241112152307.png]]
#### 参数学习
- 这里使用交叉熵模型作为损失函数,那么可以得到风险函数如下:
![[Pasted image 20241112152349.png]]
- 利用梯度下降算法求解参数,这里需要求解风险函数的梯度,求解过程如下:
![[Pasted image 20241112152524.png]]
- 从而利用梯度下降算法进行参数的迭代更新就可以求解得到最优的矩阵 $\mathbf W$ 
![[Pasted image 20241112152608.png]]
### 感知器
- 感知器是一种最简单的人工神经网络,只有一个神经元,对于外界的特征信息只会输出`1`或者`-1` , 所以最终的分类准则为:
$$
\widehat y = sgn({\mathbf \omega}^T{\mathbf x})
$$
#### 参数学习
- 学习策略:
![[Pasted image 20241112155227.png]]
- 学习策略可以抽象为如下的代码:
![[Pasted image 20241112155249.png]]
- 参数学习的过程(注意注意这里使用了梯度下降算法,每一次需要利用向量的运算规则来决定新的向量需要指向哪一个位置):
![[Pasted image 20241112155326.png]]
#### 参数平均感知器
- 由于这一种利用错误数据决定变化方向的规则会使得后面的错误数据相对于前面的错误数据的影响更大,所以引出一种利用参数平均感知器也就是给每一个迭代的到的模型赋予一个权值,从而利用这一个权值得到最终的结果,同时又可以利用平均值
![[Pasted image 20241112155613.png]]
- 如果扩展到多分类,那么就需要建立一个广义的感知机模型:
![[Pasted image 20241112155701.png]]
### 支持向量机
- 支持向量机也是一种二分类算法,也是为了寻找一个超平面对于样本进行分类,这里寻找的方法就是不断计算样本距离目前的模型的平面的距离,从而不断调整模型的参数(比如利用梯度下降算法等方式)(注意训练的过程是参数不断更新的过程,需要不断调整模型的参数)
![[Pasted image 20241112161426.png]]
![[Pasted image 20241112161437.png]]
#### 参数学习
- 一般利用拉格朗日法进行参数的学习(注意随着训练集的数据的不断读取,模型参数会不断调整)
![[Pasted image 20241112161540.png]]
- 利用拉格朗日对偶函数就可以求解最优的参数 $y^*$ 了,知道最优的参数之后可以使用支持向量的任意一个点求解偏置`b`
![[Pasted image 20241112161647.png]]
### 核函数和软间隔
- 核函数用于把原始空间映射到更加高维度的空间,这一个过程中需要使用核函数
![[Pasted image 20241112161902.png]]
- 软间隔: 类似于参数的正则化,放置样本在线性不可分开的区间里面无法找到最优解
![[Pasted image 20241112161915.png]]
### 损失函数对比
- `Logistic`的损失函数利用了交叉熵损失函数
- 感知器损失函数
- 支持向量器损失函数
- 平方损失函数
- 效果对比(最终得到的模型的`yf(x;w)`大并且损失函数的值小就是比较好的模型):
![[Pasted image 20241112162018.png]]
- 几种线性模型的对比:
![[Pasted image 20241112162111.png]]
# 基本模型
## 前馈神经网络
### 神经元
- 人工神经元是对于生物神经元的一种模拟,生物神经元通过树突接受信息,通过突触发送信息给之后的神经元,所以人工神经元也是一样的(接受信息,输入信号到达一定的阈值之后就会使得神经元处于激活状态,从而产生电脉冲完成信息的传递)
#### 激活函数
- 假设一个神经元接受$D$个输入$x_1,x_2,x_3 ... , x_D$ ,可以利用向量 $\mathbf x = [x_1,x_2,x_3, ... , x_D]$ 来表示输入,并且使用净输入量来表示一个神经元获得的输入信息的加权和
$$
z = \sum_\limits{d = 1}^D{\omega}_d{x}_d + b = {\mathbf \omega}^T{\mathbf x} + b
$$
- `z`经过一个非线性函数`f(.)`之后就可以得到神经元的活性值`a`,其中 $a = f(z)$ ,非线性函数`f(.)` 就是激活函数
![[Pasted image 20241112165944.png]]
##### Sigmoid函数
- 一种`S`型曲线函数,两端饱和函数(也就是两个极限位置都是趋于一个常数)
- `Logistic`函数,定义如下:
$$
\sigma(x) = \frac{1}{1 + exp(-x)}
$$
- 性质如下:
$$
\sigma^{'}(x) = \sigma(x)(1 - \sigma(x))
$$
- 当信号比较小的时候,活性基本为`0`,信号比较大的时候活性接近与`1`
- `Tanh`函数,定义如下:
$$
tanh(x) = \frac{exp(x) - exp(-x)}{exp(x) + exp(-x)}
$$
- 也就是:
$$
tanh(x) = 2\sigma(2x) - 1
$$
- `Tanh`函数的输出是零中心化的,`Logistic`函数的输出恒大于`0`,非零中心化的输出会使得后面一层的神经元的输入发生偏置从而使得梯度下降的收敛速度变慢 
![[Pasted image 20241112170758.png]]
- `Hard Logistic`函数和`Hard Tanh`函数,都是一种近似的线性分段函数
![[Pasted image 20241112170903.png]]
##### ReLU函数
- 一种斜坡函数,定义如下:
$$
ReLU(x) =
\begin{cases} 
x & \text{if } x >= 0 \\
0 & \text{if } x < 0 
\end{cases} = max(0,x)
$$
- 优点: 操作简单,稀疏性比较好(分布均匀),大于`0`时 导数为`1`
- 缺点: 非零中心化,如果一个神经元接收到负信号就会死亡无法被激活
- 带泄漏的`ReLU` 和带参数的`ReLU`
![[Pasted image 20241112171414.png]]
##### ELU函数和Softplus函数
![[Pasted image 20241112171507.png]]
##### Swish函数
- 一种自门空控激活函数
![[Pasted image 20241112171550.png]]
##### Maxout单元
- 用于描述上一层神经元的整体输出,
![[Pasted image 20241112171754.png]]

